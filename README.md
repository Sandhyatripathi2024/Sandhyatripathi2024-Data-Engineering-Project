Utilized advanced data engineering techniques to design and implement a scalable data pipeline using Hadoop and Docker, enabling efficient and systematic data management. The data was sourced through web scraping, followed by preprocessing using Python. The cleaned data was then stored in HDFS (Hadoop Distributed File System) for distributed storage.
After data ingestion into HDFS, the dataset was loaded into a MySQL database, where an entity-relationship (E-R) diagram was created, and the schema for the database was designed. MySQL was utilized for structured data storage and complex querying, facilitating business intelligence and decision-making processes.
The project was deployed using Oracle VirtualBox, with three virtual machines running Windows and one Mac machine. The Mac machine was designated as the master node, while the remaining Windows machines served as worker nodes. A secure master-worker configuration was established using Tailscale to connect all machines within a virtual private network (VPN). Hadoop was installed on the master node, and the data files were stored in HDFS.
Redundancy testing was conducted to ensure system resilience and fault tolerance. This robust data pipeline provided a reliable infrastructure for managing and processing large datasets, generating actionable business insights from the MySQL database.
